
-- Elastisticy Example - libCEED + PETSc --
  libCEED:
    libCEED Backend                    : /cpu/self/xsmm/blocked
  Problem:
    Problem Name                       : Hyper elasticity small strain
    Forcing Function                   : None
  Mesh:
    File                               : ./meshes/cyl-hole_1994e_4ss_us.exo
    Number of 1D Basis Nodes (p)       : 4
    Number of 1D Quadrature Points (q) : 4
    Global nodes                       : 71154
    Owned nodes                        : 146
    DoF per node                       : 3
  Multigrid:
    Type                               : P-multigrid, logarithmic coarsening
    Number of Levels                   : 3
    Level 0 (coarse):
      Number of 1D Basis Nodes (p)     : 2
      Global Nodes                     : 3212
      Owned Nodes                      : 0
    Level 2 (fine):
      Number of 1D Basis Nodes (p)     : 4
      Global Nodes                     : 71154
      Owned Nodes                      : 146
0 Load Increment
  0 SNES Function norm 7.175256269954e+05 
  1 SNES Function norm 1.662011875841e+01 
  2 SNES Function norm 2.628583797572e-04 
SNES Object: 192 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=74
  total number of function evaluations=5
  norm schedule ALWAYS
  SNESLineSearch Object: 192 MPI processes
    type: cp
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object: (outer_) 192 MPI processes
    type: cg
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using NATURAL norm type for convergence test
  PC Object: (outer_) 192 MPI processes
    type: mg
      type is MULTIPLICATIVE, levels=3 cycles=v
        Cycles per PCApply=1
        Not using Galerkin computed coarse grid matrices
    Coarse grid solver -- level -------------------------------
      KSP Object: (coarse_) 192 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (coarse_) 192 MPI processes
        type: gamg
          type is MULTIPLICATIVE, levels=3 cycles=v
            Cycles per PCApply=1
            Using externally compute Galerkin coarse grid matrices
            GAMG specific options
              Threshold for dropping small values in graph on each level =         0.        
              Threshold scaling factor for each level not specified = 1.
              AGG specific options
                Symmetric graph false
                Number of levels to square graph 1
                Number smoothing steps 1
              Complexity:    grid = 1.02563
        Coarse grid solver -- level -------------------------------
          KSP Object: (coarse_mg_coarse_) 192 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_coarse_) 192 MPI processes
            type: bjacobi
              number of blocks = 192
              Local solver is the same for all blocks, as in the following KSP and PC objects on rank 0:
            KSP Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 5., needed 1.13208
                  Factored matrix follows:
                    Mat Object: 1 MPI processes
                      type: seqaij
                      rows=36, cols=36, bs=3
                      package used to perform factorization: petsc
                      total: nonzeros=1080, allocated nonzeros=1080
                      total number of mallocs used during MatSetValues calls=0
                        using I-node routines: found 9 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=3
                total: nonzeros=954, allocated nonzeros=954
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 12 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=36, cols=36, bs=3
              total: nonzeros=954, allocated nonzeros=954
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 12 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (coarse_mg_levels_1_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.160398, max = 1.76438
              eigenvalues estimate via gmres min 0.0380404, max 1.60398
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_1_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_1_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=291, cols=291, bs=3
              total: nonzeros=14967, allocated nonzeros=14967
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 6 nodes, limit used is 5
        Up solver (post-smoother) same as down solver (pre-smoother)
        Down solver (pre-smoother) on level 2 -------------------------------
          KSP Object: (coarse_mg_levels_2_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.21775, max = 2.39525
              eigenvalues estimate via gmres min 0.0272134, max 2.1775
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_2_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_2_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=9636, cols=9636, bs=3
              total: nonzeros=621270, allocated nonzeros=621270
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: mpiaij
          rows=9636, cols=9636, bs=3
          total: nonzeros=621270, allocated nonzeros=621270
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
    Down solver (pre-smoother) on level 1 -------------------------------
      KSP Object: (outer_mg_levels_1_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.289175, max = 3.18093
          eigenvalues estimate via cg min 0.05052, max 2.89175
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_1_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_1_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=66750, cols=66750
    Up solver (post-smoother) same as down solver (pre-smoother)
    Down solver (pre-smoother) on level 2 -------------------------------
      KSP Object: (outer_mg_levels_2_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.327348, max = 3.60083
          eigenvalues estimate via cg min 0.0611565, max 3.27348
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_2_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_2_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=213462, cols=213462
    Up solver (post-smoother) same as down solver (pre-smoother)
    linear system matrix = precond matrix:
    Mat Object: 192 MPI processes
      type: shell
      rows=213462, cols=213462
1 Load Increment
  0 SNES Function norm 7.175256269970e+05 
  1 SNES Function norm 1.665130010121e+01 
  2 SNES Function norm 2.731886890800e-04 
SNES Object: 192 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=75
  total number of function evaluations=5
  norm schedule ALWAYS
  SNESLineSearch Object: 192 MPI processes
    type: cp
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object: (outer_) 192 MPI processes
    type: cg
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using NATURAL norm type for convergence test
  PC Object: (outer_) 192 MPI processes
    type: mg
      type is MULTIPLICATIVE, levels=3 cycles=v
        Cycles per PCApply=1
        Not using Galerkin computed coarse grid matrices
    Coarse grid solver -- level -------------------------------
      KSP Object: (coarse_) 192 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (coarse_) 192 MPI processes
        type: gamg
          type is MULTIPLICATIVE, levels=3 cycles=v
            Cycles per PCApply=1
            Using externally compute Galerkin coarse grid matrices
            GAMG specific options
              Threshold for dropping small values in graph on each level =         0.        
              Threshold scaling factor for each level not specified = 1.
              AGG specific options
                Symmetric graph false
                Number of levels to square graph 1
                Number smoothing steps 1
              Complexity:    grid = 1.02563
        Coarse grid solver -- level -------------------------------
          KSP Object: (coarse_mg_coarse_) 192 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_coarse_) 192 MPI processes
            type: bjacobi
              number of blocks = 192
              Local solver is the same for all blocks, as in the following KSP and PC objects on rank 0:
            KSP Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 5., needed 1.13208
                  Factored matrix follows:
                    Mat Object: 1 MPI processes
                      type: seqaij
                      rows=36, cols=36, bs=3
                      package used to perform factorization: petsc
                      total: nonzeros=1080, allocated nonzeros=1080
                      total number of mallocs used during MatSetValues calls=0
                        using I-node routines: found 9 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=3
                total: nonzeros=954, allocated nonzeros=954
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 12 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=36, cols=36, bs=3
              total: nonzeros=954, allocated nonzeros=954
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 12 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (coarse_mg_levels_1_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.160395, max = 1.76435
              eigenvalues estimate via gmres min 0.0380395, max 1.60395
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_1_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_1_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=291, cols=291, bs=3
              total: nonzeros=14967, allocated nonzeros=14967
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 6 nodes, limit used is 5
        Up solver (post-smoother) same as down solver (pre-smoother)
        Down solver (pre-smoother) on level 2 -------------------------------
          KSP Object: (coarse_mg_levels_2_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.21766, max = 2.39426
              eigenvalues estimate via gmres min 0.0272045, max 2.1766
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_2_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_2_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=9636, cols=9636, bs=3
              total: nonzeros=621270, allocated nonzeros=621270
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: mpiaij
          rows=9636, cols=9636, bs=3
          total: nonzeros=621270, allocated nonzeros=621270
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
    Down solver (pre-smoother) on level 1 -------------------------------
      KSP Object: (outer_mg_levels_1_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.289196, max = 3.18116
          eigenvalues estimate via cg min 0.0505233, max 2.89196
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_1_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_1_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=66750, cols=66750
    Up solver (post-smoother) same as down solver (pre-smoother)
    Down solver (pre-smoother) on level 2 -------------------------------
      KSP Object: (outer_mg_levels_2_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.327351, max = 3.60086
          eigenvalues estimate via cg min 0.0611556, max 3.27351
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_2_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_2_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=213462, cols=213462
    Up solver (post-smoother) same as down solver (pre-smoother)
    linear system matrix = precond matrix:
    Mat Object: 192 MPI processes
      type: shell
      rows=213462, cols=213462
2 Load Increment
  0 SNES Function norm 7.175256269995e+05 
  1 SNES Function norm 1.671101102252e+01 
  2 SNES Function norm 2.920443143501e-04 
SNES Object: 192 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=75
  total number of function evaluations=5
  norm schedule ALWAYS
  SNESLineSearch Object: 192 MPI processes
    type: cp
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object: (outer_) 192 MPI processes
    type: cg
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using NATURAL norm type for convergence test
  PC Object: (outer_) 192 MPI processes
    type: mg
      type is MULTIPLICATIVE, levels=3 cycles=v
        Cycles per PCApply=1
        Not using Galerkin computed coarse grid matrices
    Coarse grid solver -- level -------------------------------
      KSP Object: (coarse_) 192 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (coarse_) 192 MPI processes
        type: gamg
          type is MULTIPLICATIVE, levels=3 cycles=v
            Cycles per PCApply=1
            Using externally compute Galerkin coarse grid matrices
            GAMG specific options
              Threshold for dropping small values in graph on each level =         0.        
              Threshold scaling factor for each level not specified = 1.
              AGG specific options
                Symmetric graph false
                Number of levels to square graph 1
                Number smoothing steps 1
              Complexity:    grid = 1.02563
        Coarse grid solver -- level -------------------------------
          KSP Object: (coarse_mg_coarse_) 192 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_coarse_) 192 MPI processes
            type: bjacobi
              number of blocks = 192
              Local solver is the same for all blocks, as in the following KSP and PC objects on rank 0:
            KSP Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 5., needed 1.13208
                  Factored matrix follows:
                    Mat Object: 1 MPI processes
                      type: seqaij
                      rows=36, cols=36, bs=3
                      package used to perform factorization: petsc
                      total: nonzeros=1080, allocated nonzeros=1080
                      total number of mallocs used during MatSetValues calls=0
                        using I-node routines: found 9 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=3
                total: nonzeros=954, allocated nonzeros=954
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 12 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=36, cols=36, bs=3
              total: nonzeros=954, allocated nonzeros=954
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 12 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (coarse_mg_levels_1_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.160393, max = 1.76432
              eigenvalues estimate via gmres min 0.0380387, max 1.60393
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_1_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_1_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=291, cols=291, bs=3
              total: nonzeros=14967, allocated nonzeros=14967
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 6 nodes, limit used is 5
        Up solver (post-smoother) same as down solver (pre-smoother)
        Down solver (pre-smoother) on level 2 -------------------------------
          KSP Object: (coarse_mg_levels_2_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.217572, max = 2.39329
              eigenvalues estimate via gmres min 0.027196, max 2.17572
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_2_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_2_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=9636, cols=9636, bs=3
              total: nonzeros=621270, allocated nonzeros=621270
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: mpiaij
          rows=9636, cols=9636, bs=3
          total: nonzeros=621270, allocated nonzeros=621270
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
    Down solver (pre-smoother) on level 1 -------------------------------
      KSP Object: (outer_mg_levels_1_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.289234, max = 3.18157
          eigenvalues estimate via cg min 0.0505306, max 2.89234
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_1_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_1_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=66750, cols=66750
    Up solver (post-smoother) same as down solver (pre-smoother)
    Down solver (pre-smoother) on level 2 -------------------------------
      KSP Object: (outer_mg_levels_2_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.327356, max = 3.60092
          eigenvalues estimate via cg min 0.0611551, max 3.27356
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_2_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_2_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=213462, cols=213462
    Up solver (post-smoother) same as down solver (pre-smoother)
    linear system matrix = precond matrix:
    Mat Object: 192 MPI processes
      type: shell
      rows=213462, cols=213462
  SNES:
    SNES Type                          : newtonls
    SNES Convergence                   : CONVERGED_FNORM_RELATIVE
    Number of Load Increments          : 3
    Completed Load Increments          : 3
    Total SNES Iterations              : 6
    Final rnorm                        : 2.920443e-04
  Linear Solver:
    KSP Type                           : cg
    PC Type                            : mg
  P-Multigrid:
    PCMG Type                          : MULTIPLICATIVE
    PCMG Cycle Type                    : v
    Coarse Solve:
      KSP Type                         : preonly
      PC Type                          : gamg
  Performance:
    SNES Solve Time                    : 1.12298 (1.12235) sec
    Strain Energy                      : -1.358801e+07
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./elasticity on a ompi-gcc8-O3 named shas0102.rc.int.colorado.edu with 192 processors, by arme5062 Thu Nov 19 16:45:57 2020
Using Petsc Development GIT revision: v3.13.1-148-g80ec162  GIT Date: 2020-05-08 19:48:40 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.621e+00     1.007   1.620e+00
Objects:              7.358e+03     1.093   6.824e+03
Flop:                 2.402e+08     1.467   1.879e+08  3.607e+10
Flop/sec:             1.482e+08     1.466   1.160e+08  2.227e+10
MPI Messages:         1.402e+05     6.149   7.872e+04  1.511e+07
MPI Message Lengths:  6.349e+07     4.642   4.450e+02  6.727e+09
MPI Reductions:       4.160e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 8.9230e-03   0.6%  2.3349e+07   0.1%  2.031e+03   0.0%  5.162e+02        0.0%  5.000e+00   0.1% 
 1: DM and Vector Setup Stage: 4.6557e-01  28.7%  2.9714e+10  82.4%  7.560e+04   0.5%  1.216e+02        0.1%  2.050e+02   4.9% 
 2: libCEED Setup Stage: 4.5000e-03   0.3%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 3: SNES Setup Stage: 1.7869e-02   1.1%  1.9452e+05   0.0%  3.671e+04   0.2%  7.907e+02        0.4%  3.700e+01   0.9% 
 4: SNES Solve Stage: 1.1232e+00  69.3%  6.3333e+09  17.6%  1.500e+07  99.2%  4.458e+02       99.4%  3.906e+03  93.9% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult               10 0.0 4.9340e-06 0.0 1.92e+03 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4047
SFBcastOpBegin         1 1.0 3.2119e-0514.7 0.00e+00 0.0 1.3e+03 8.3e+02 0.0e+00  0  0  0  0  0   0  0 62100  0     0
SFBcastOpEnd           1 1.0 1.2153e-03641.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
SFPack                 1 1.0 6.1340e-0653.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               1 1.0 1.2316e-05155.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 3 1.0 1.5292e-05 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 1: DM and Vector Setup Stage

BuildTwoSided         20 1.0 4.7036e-02 4.4 0.00e+00 0.0 1.6e+04 4.0e+00 2.0e+01  2  0  0  0  0   7  0 21  1 10     0
MatAssemblyBegin      98 1.0 2.3077e-05 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        98 1.0 2.5714e-04 2.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexCrFromFile       1 1.0 4.5298e-02 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 5.0e+00  2  0  0  0  0   8  0  0  0  2     0
Mesh Partition         1 1.0 3.0716e-01 1.0 0.00e+00 0.0 9.6e+02 9.2e+02 1.1e+01 19  0  0  0  0  66  0  1 10  5     0
Mesh Migration         1 1.0 8.0298e-03 1.0 0.00e+00 0.0 6.5e+03 6.3e+02 3.6e+01  0  0  0  0  1   2  0  9 44 18     0
DMPlexPartSelf         1 1.0 6.9639e-031123.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexPartLblInv       1 1.0 1.7754e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 3.0e+00 10  0  0  0  0  36  0  0  2  1     0
DMPlexPartLblSF        1 1.0 2.8371e-02 5.6 0.00e+00 0.0 3.8e+02 4.6e+02 1.0e+00  1  0  0  0  0   4  0  1  2  0     0
DMPlexPartStrtSF       1 1.0 1.3864e-03 1.8 0.00e+00 0.0 1.9e+02 1.8e+03 0.0e+00  0  0  0  0  0   0  0  0  4  0     0
DMPlexPointSF          1 1.0 2.7755e-03 1.0 0.00e+00 0.0 3.8e+02 2.3e+03 0.0e+00  0  0  0  0  0   1  0  1 10  0     0
DMPlexInterp          40 1.0 2.9771e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 2.3e+01  2  0  0  0  1   6  0  0  0 11     0
DMPlexDistribute       1 1.0 3.1802e-01 1.0 0.00e+00 0.0 7.8e+03 7.5e+02 4.7e+01 20  0  0  0  1  68  0 10 64 23     0
DMPlexDistCones        1 1.0 1.8777e-03 1.0 0.00e+00 0.0 1.1e+03 1.4e+03 2.0e+00  0  0  0  0  0   0  0  2 17  1     0
DMPlexDistLabels       1 1.0 4.7088e-03 1.1 0.00e+00 0.0 3.8e+03 4.7e+02 3.1e+01  0  0  0  0  1   1  0  5 20 15     0
DMPlexDistField        1 1.0 1.4371e-03 1.4 0.00e+00 0.0 1.3e+03 4.0e+02 2.0e+00  0  0  0  0  0   0  0  2  6  1     0
DMPlexStratify        55 1.0 3.0666e-02 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 5.5e+01  2  0  0  0  1   6  0  0  0 27     0
DMPlexSymmetrize      55 1.0 1.8917e-0336.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph            21 1.0 1.7453e-05 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               20 1.0 5.0073e-02 4.1 0.00e+00 0.0 3.1e+04 5.7e+01 2.0e+01  2  0  0  0  0   8  0 41 20 10     0
SFBcastOpBegin        49 1.0 3.5134e-0321.8 0.00e+00 0.0 3.5e+04 1.7e+02 0.0e+00  0  0  0  0  0   0  0 47 63  0     0
SFBcastOpEnd          49 1.0 9.3609e-03 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   2  0  0  0  0     0
SFReduceBegin          6 1.0 2.1550e-04 4.5 0.00e+00 0.0 6.5e+03 1.9e+02 0.0e+00  0  0  0  0  0   0  0  9 13  0     0
SFReduceEnd            6 1.0 2.5449e-02242.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
SFFetchOpBegin         1 1.0 2.0865e-0517.9 0.00e+00 0.0 1.3e+03 6.8e+01 0.0e+00  0  0  0  0  0   0  0  2  1  0     0
SFFetchOpEnd           1 1.0 5.7850e-03104.1 0.00e+00 0.0 1.3e+03 6.8e+01 0.0e+00  0  0  0  0  0   0  0  2  1  0     0
SFCreateEmbed          1 1.0 2.8514e-0431.0 0.00e+00 0.0 1.9e+02 2.3e+02 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFDistSection         12 1.0 2.4196e-02 5.6 0.00e+00 0.0 2.5e+04 1.3e+02 1.5e+01  1  0  0  0  0   4  0 34 35  7     0
SFSectionSF           12 1.0 3.7757e-03 1.6 0.00e+00 0.0 1.5e+04 5.8e+01 1.2e+01  0  0  0  0  0   1  0 20 10  6     0
SFPack                56 1.0 1.3892e-03173.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              57 1.0 1.7880e-03194.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                55 1.0 8.1208e-05 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 2: libCEED Setup Stage

VecSet                 4 1.0 4.3260e-06 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 3: SNES Setup Stage

BuildTwoSided          8 1.0 3.6330e-03 4.7 0.00e+00 0.0 9.3e+03 4.0e+00 8.0e+00  0  0  0  0  0  15  0 25  0 22     0
BuildTwoSidedF         1 1.0 7.5458e-0415.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   4  0  0  0  3     0
MatAssemblyBegin       1 1.0 7.7558e-04 8.5 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   4  0  0  0  3     0
MatAssemblyEnd         1 1.0 1.3656e-03 1.0 0.00e+00 0.0 4.2e+03 3.6e+01 5.0e+00  0  0  0  0  0   8  0 11  1 14     0
DMCreateMat            1 1.0 1.0304e-02 1.0 0.00e+00 0.0 2.7e+04 9.5e+02 1.9e+01  1  0  0  0  0  58  0 72 87 51     0
DMPlexPrealloc         1 1.0 1.0143e-02 1.0 0.00e+00 0.0 2.7e+04 9.5e+02 1.7e+01  1  0  0  0  0  57  0 72 87 46     0
SFSetGraph             9 1.0 1.2811e-05 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                7 1.0 3.3288e-03 2.4 0.00e+00 0.0 1.9e+04 5.0e+02 7.0e+00  0  0  0  0  0  14  0 51 32 19     0
SFBcastOpBegin         7 1.0 3.5464e-0412.6 0.00e+00 0.0 8.6e+03 5.2e+02 0.0e+00  0  0  0  0  0   1  0 23 15  0     0
SFBcastOpEnd           7 1.0 1.6312e-03140.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   4  0  0  0  0     0
SFReduceBegin          6 1.0 2.3351e-04 6.3 0.00e+00 0.0 7.2e+03 1.4e+03 0.0e+00  0  0  0  0  0   1  0 20 34  0     0
SFReduceEnd            6 1.0 2.1402e-0384.8 3.40e+03 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   4100  0  0  0    91
SFFetchOpBegin         1 1.0 3.1454e-0515.7 0.00e+00 0.0 1.2e+03 2.3e+03 0.0e+00  0  0  0  0  0   0  0  3  9  0     0
SFFetchOpEnd           1 1.0 5.6323e-0411.2 0.00e+00 0.0 1.2e+03 2.3e+03 0.0e+00  0  0  0  0  0   1  0  3  9  0     0
SFCreateEmbed          2 1.0 9.8428e-0418.4 0.00e+00 0.0 2.4e+03 1.7e+01 0.0e+00  0  0  0  0  0   2  0  7  0  0     0
SFSectionSF            2 1.0 1.3006e-03 3.1 0.00e+00 0.0 4.7e+03 6.0e+02 2.0e+00  0  0  0  0  0   5  0 13 10  5     0
SFRemoteOff            2 1.0 1.2269e-0310.8 0.00e+00 0.0 4.9e+03 4.3e+01 0.0e+00  0  0  0  0  0   3  0 13  1  0     0
SFPack                14 1.0 6.5642e-05 5.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              15 1.0 2.4219e-0415.8 3.40e+03 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0100  0  0  0   803
VecSet                14 1.0 3.9748e-05 3.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 4: SNES Solve Stage

PetscBarrier           2 1.0 9.1736e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   1  0  0  0  0     0
BuildTwoSided        367 1.0 1.2988e-01 1.8 0.00e+00 0.0 1.1e+05 4.0e+00 3.7e+02  7  0  1  0  9  10  0  1  0  9     0
BuildTwoSidedF       252 1.0 1.2294e-01 1.9 0.00e+00 0.0 4.3e+04 1.2e+03 2.5e+02  6  0  0  1  6   9  0  0  1  6     0
MatMult             5794 1.0 4.9617e-01 1.3 3.28e+0755.8 1.1e+07 5.1e+02 0.0e+00 27  5 74 86  0  38 28 75 86  0  3583
MatMultAdd           920 1.0 1.4868e-01 3.2 3.27e+06 3.6 1.2e+06 4.2e+02 0.0e+00  6  1  8  8  0   8  5  8  8  0  2187
MatMultTranspose     928 1.0 8.6198e-02 2.8 1.65e+06 3.3 1.2e+06 4.3e+02 0.0e+00  3  0  8  8  0   4  3  8  8  0  2029
MatSolve             230 0.0 4.5107e-04 0.0 4.89e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1083
MatSOR              1972 1.0 9.6437e-0335.0 1.60e+07 0.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  9  0  0  0 58666
MatLUFactorSym         6 1.0 2.3573e-04 6.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         6 1.0 1.0979e-0416.9 1.21e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1105
MatScale              36 1.0 7.1993e-04 7.7 3.24e+04 0.0 1.3e+04 4.6e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0  2664
MatResidual          920 1.0 7.7019e-02 1.6 7.21e+0646.3 1.7e+06 4.7e+02 0.0e+00  4  1 11 12  0   6  6 11 12  0  5218
MatAssemblyBegin     331 1.0 1.1772e-01 1.9 0.00e+00 0.0 4.3e+04 1.2e+03 1.1e+02  6  0  0  1  3   9  0  0  1  3     0
MatAssemblyEnd       331 1.0 7.0077e-02 1.3 4.77e+04 0.0 1.3e+05 1.5e+01 4.5e+02  4  0  1  0 11   6  0  1  0 12    35
MatGetRowIJ            7 7.0 1.3873e-03 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMat       24 1.0 3.1316e-02 1.0 0.00e+00 0.0 7.1e+03 1.8e+02 3.4e+02  2  0  0  0  8   3  0  0  0  9     0
MatGetOrdering         6 0.0 1.8869e-04 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCoarsen            12 1.0 9.9939e-03 1.1 0.00e+00 0.0 2.8e+05 3.2e+01 8.4e+01  1  0  2  0  2   1  0  2  0  2     0
MatZeroEntries        18 1.0 7.5158e-05 8.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView               27 1.3 9.0666e-03 2.7 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  1  0  0  0  0   1  0  0  0  0     0
MatAXPY               12 1.0 2.2620e-03 2.4 3.46e+03 0.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0  0    79
MatFDColorCreate       1 1.0 1.4033e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatFDColorSetUp        1 1.0 1.3430e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.2e+02  1  0  0  0 10   1  0  0  0 11     0
MatFDColorApply        6 1.0 6.5400e-02 1.0 5.63e+05 0.0 1.5e+06 1.4e+02 9.0e+00  4  0 10  3  0   6  0 10  3  0   384
MatFDColorFunc       630 1.0 6.4498e-02 1.0 3.21e+05 0.0 1.5e+06 1.4e+02 3.0e+00  4  0 10  3  0   6  0 10  3  0   201
MatTranspose          24 1.0 5.7752e-04 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultSym         36 1.0 1.9132e-02 1.1 0.00e+00 0.0 5.0e+04 1.7e+02 1.2e+02  1  0  0  0  3   2  0  0  0  3     0
MatMatMultNum         12 1.0 2.9500e-03 1.4 4.54e+05 0.0 1.3e+04 4.1e+02 1.2e+01  0  0  0  0  0   0  0  0  0  0  7764
MatPtAPSymbolic       12 1.0 2.0993e-02 1.1 0.00e+00 0.0 5.7e+04 4.9e+02 8.4e+01  1  0  0  0  2   2  0  0  0  2     0
MatPtAPNumeric        12 1.0 1.1881e-02 1.1 1.66e+06 0.0 2.4e+04 3.6e+02 7.2e+01  1  0  0  0  2   1  1  0  0  2  7692
MatTrnMatMultSym       6 1.0 1.0175e-02 1.0 0.00e+00 0.0 3.8e+04 2.8e+02 4.2e+01  1  0  0  0  1   1  0  0  0  1     0
MatTrnMatMultNum       6 1.0 1.5891e-02 1.0 4.63e+05 0.0 6.8e+04 5.9e+02 3.6e+01  1  0  0  1  1   1  0  0  1  1  1508
MatGetLocalMat        42 1.0 7.0308e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetBrAoCol         36 1.0 6.3205e-0313.3 0.00e+00 0.0 8.9e+04 4.4e+02 0.0e+00  0  0  1  1  0   0  0  1  1  0     0
MatColoringApply       1 1.0 3.3469e-02 1.0 0.00e+00 0.0 1.9e+02 4.0e+00 3.0e+00  2  0  0  0  0   3  0  0  0  0     0
DMPlexInterp          13 1.0 1.4241e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 7.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify        17 1.0 2.2886e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.7e+01  0  0  0  0  0   0  0  0  0  0     0
DMPlexSymmetrize      17 1.0 3.6402e-05 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph           115 1.0 3.7203e-05 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp              115 1.0 1.3122e-02 1.8 0.00e+00 0.0 1.7e+05 1.6e+01 1.2e+02  1  0  1  0  3   1  0  1  0  3     0
SFBcastOpBegin      7487 1.0 7.6679e-02 9.8 0.00e+00 0.0 8.7e+06 4.0e+02 0.0e+00  2  0 58 51  0   3  0 58 52  0     0
SFBcastOpEnd        7487 1.0 3.4517e-01 3.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 14  0  0  0  0  20  0  0  0  0     0
SFReduceBegin       5149 1.0 5.3773e-02 5.0 0.00e+00 0.0 5.9e+06 5.3e+02 0.0e+00  2  0 39 46  0   3  0 40 47  0     0
SFReduceEnd         5149 1.0 2.4688e-0112.3 6.96e+06 0.0 0.0e+00 0.0e+00 0.0e+00  9  1  0  0  0  13  6  0  0  0  1584
SFPack             12636 1.0 9.6616e-03 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack           12636 1.0 1.1533e-02 3.8 6.96e+06 0.0 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   1  6  0  0  0 33897
VecDot                12 1.0 4.2258e-0311.5 4.74e+04 4.5 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0  0  1212
VecMDot              240 1.0 1.4042e-02 2.1 2.53e+05 0.0 0.0e+00 0.0e+00 2.4e+02  1  0  0  0  6   1  0  0  0  6   924
VecTDot              706 1.0 1.1116e-01 2.0 2.48e+06 4.7 0.0e+00 0.0e+00 7.1e+02  4  1  0  0 17   6  4  0  0 18  2378
VecNorm              399 1.0 1.6883e-02 1.6 4.24e+05 5.6 0.0e+00 0.0e+00 4.0e+02  1  0  0  0 10   1  1  0  0 10  2422
VecScale            4308 1.0 4.8617e-04 1.8 2.53e+04 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2695
VecCopy             2554 1.0 1.3445e-03 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet             17245 1.0 1.2824e-02 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
VecAXPY             1829 1.0 2.2891e-03 3.5 4.04e+06 5.2 0.0e+00 0.0e+00 0.0e+00  0  1  0  0  0   0  6  0  0  0 179682
VecAYPX             6766 1.0 4.3092e-03 3.5 7.81e+06 5.3 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0 12  0  0  0 182421
VecAXPBYCZ          2760 1.0 2.7743e-03 3.4 1.31e+07 5.4 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0 21  0  0  0 472840
VecMAXPY             264 1.0 1.3970e-04 2.8 3.00e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 110854
VecAssemblyBegin     156 1.0 6.0655e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.4e+02  0  0  0  0  3   0  0  0  0  4     0
VecAssemblyEnd       156 1.0 1.3512e-04 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult    3958 1.0 5.5363e-03 3.3 5.26e+06 2.6 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0 10  0  0  0 113783
VecScatterBegin     3174 1.0 2.7712e-0217.4 0.00e+00 0.0 2.7e+06 1.2e+02 0.0e+00  1  0 18  5  0   1  0 18  5  0     0
VecScatterEnd       3174 1.0 1.1429e-0197.6 2.07e+05 0.0 0.0e+00 0.0e+00 0.0e+00  6  0  0  0  0   9  0  0  0  0     8
VecSetRandom          12 1.0 4.5662e-0516.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecReduceArith        18 1.0 1.7704e-05 2.7 7.10e+04 4.5 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 433867
VecReduceComm          6 1.0 3.7235e-0340.0 0.00e+00 0.0 0.0e+00 0.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         264 1.0 7.3806e-03 1.3 7.60e+04 0.0 0.0e+00 0.0e+00 2.6e+02  0  0  0  0  6   1  0  0  0  7   533
SNESSolve              3 1.0 1.1021e+00 1.0 8.55e+07 9.5 1.5e+07 4.5e+02 3.9e+03 68 18 99 99 93  98100100100 99  5747
SNESSetUp              1 1.0 4.9706e-05 8.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SNESFunctionEval     645 1.0 9.0082e-02 1.2 1.38e+0727991.1 1.5e+06 1.6e+02 3.8e+01  5  1 10  4  1   7  6 10  4  1  4055
SNESJacobianEval       6 1.0 1.1243e-01 1.0 5.63e+05 0.0 1.5e+06 1.4e+02 4.3e+02  7  0 10  3 10  10  0 10  3 11   223
SNESLineSearch         6 1.0 1.1164e-02 1.0 1.11e+07234.6 3.0e+04 8.3e+02 1.8e+01  1  1  0  0  0   1  5  0  0  0 27302
KSPSetUp              74 1.0 1.4853e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 2.8e+01  0  0  0  0  1   0  0  0  0  1     0
KSPSolve               6 1.0 9.6059e-01 1.0 8.46e+07 9.5 1.3e+07 4.8e+02 3.4e+03 59 16 89 96 82  86 94 90 96 87  6174
KSPGMRESOrthog       240 1.0 1.4216e-02 2.0 5.06e+05 0.0 0.0e+00 0.0e+00 2.4e+02  1  0  0  0  6   1  0  0  0  6  1835
PCGAMGGraph_AGG       12 1.0 2.9722e-02 1.0 8.41e+04 0.0 6.4e+04 2.0e+01 2.3e+02  2  0  0  0  5   3  0  0  0  6   143
PCGAMGCoarse_AGG      12 1.0 3.9270e-02 1.0 4.63e+05 0.0 4.8e+05 1.4e+02 2.1e+02  2  0  3  1  5   3  0  3  1  5   610
PCGAMGProl_AGG        12 1.0 1.1615e-01 1.0 0.00e+00 0.0 2.3e+05 7.3e+01 4.8e+02  7  0  1  0 12  10  0  2  0 12     0
PCGAMGPOpt_AGG        12 1.0 4.0410e-02 1.0 2.30e+06 0.0 1.9e+05 1.7e+02 4.9e+02  2  0  1  0 12   4  2  1  0 13  2892
GAMG: createProl      12 1.0 2.2550e-01 1.0 2.85e+06 0.0 9.6e+05 1.2e+02 1.4e+03 14  0  6  2 34  20  2  6  2 36   643
  Graph               24 1.0 2.9439e-02 1.0 8.41e+04 0.0 6.4e+04 2.0e+01 2.3e+02  2  0  0  0  5   3  0  0  0  6   144
  MIS/Agg             12 1.0 1.0073e-02 1.1 0.00e+00 0.0 2.8e+05 3.2e+01 8.4e+01  1  0  2  0  2   1  0  2  0  2     0
  SA: col data        12 1.0 1.3407e-02 1.1 0.00e+00 0.0 2.1e+05 6.9e+01 3.6e+02  1  0  1  0  9   1  0  1  0  9     0
  SA: frmProl0        12 1.0 1.0084e-01 1.0 0.00e+00 0.0 1.1e+04 1.6e+02 7.2e+01  6  0  0  0  2   9  0  0  0  2     0
  SA: smooth          12 1.0 2.5348e-02 1.0 4.73e+05 0.0 6.3e+04 2.2e+02 1.7e+02  2  0  0  0  4   2  0  0  0  4   953
GAMG: partLevel       12 1.0 9.0047e-02 1.0 1.66e+06 0.0 9.1e+04 4.2e+02 7.9e+02  6  0  1  1 19   8  1  1  1 20  1015
  repartition         12 1.0 5.8484e-02 1.0 0.00e+00 0.0 9.8e+03 1.4e+02 6.4e+02  4  0  0  0 15   5  0  0  0 16     0
  Invert-Sort         12 1.0 7.0317e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 7.2e+01  0  0  0  0  2   1  0  0  0  2     0
  Move A              12 1.0 1.5535e-02 1.0 0.00e+00 0.0 2.5e+03 4.7e+02 1.8e+02  1  0  0  0  4   1  0  0  0  5     0
  Move P              12 1.0 1.8782e-02 1.0 0.00e+00 0.0 4.6e+03 2.2e+01 1.9e+02  1  0  0  0  5   2  0  0  0  5     0
PCSetUp               12 1.0 3.3167e-01 1.0 4.28e+06329.0 1.1e+06 1.6e+02 2.3e+03 20  1  7  3 55  29  4  7  3 59   727
PCSetUpOnBlocks      230 1.0 7.7479e-04 3.2 1.21e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   157
PCApply              230 1.0 5.5947e-01 1.1 7.54e+07 9.9 1.2e+07 4.9e+02 6.4e+02 34 14 78 86 15  49 82 79 87 16  9263
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container     0             25        14600     0.
    Distributed Mesh     0             60       309040     0.
            DM Label     0            128        81920     0.
    GraphPartitioner     0             17        11696     0.
          Quadrature     0            495       297000     0.
              Matrix     0             52       251832     0.
  Matrix FD Coloring     0              1         2036     0.
           Index Set     8            250       230968     0.
   IS L to G Mapping     0              1          940     0.
             Section     0            122        89792     0.
    Section Symmetry     0              1          696     0.
   Star Forest Graph     0             87        87744     0.
     Discrete System     0             49        45876     0.
         Vec Scatter     0              5         4080     0.
              Vector     0            126       417568     0.
        Linear Space     0              4         2752     0.
          Dual Space     0             16        14976     0.
            FE Space     0              4         3120     0.
                SNES     0              2         2820     0.
              DMSNES     0              2         1360     0.
      SNESLineSearch     0              1         1000     0.
       Krylov Solver     0             13       366808     0.
     DMKSP interface     0              1          664     0.
      Preconditioner     0              9         9804     0.
              Viewer     1              1          848     0.
       Field over DM     0              1          712     0.

--- Event Stage 1: DM and Vector Setup Stage

           Container    51             32        18688     0.
    Distributed Mesh   193            146       750048     0.
            DM Label   411            305       195200     0.
    GraphPartitioner    55             42        28896     0.
          Quadrature   706            262       157200     0.
              Matrix   149            127       402572     0.
           Index Set  1410           1311      1400872     0.
   IS L to G Mapping     1              1        76312     0.
             Section   387            292       214912     0.
    Section Symmetry     1              0            0     0.
   Star Forest Graph   406            342       341856     0.
     Discrete System   247            209       195624     0.
              Vector    61             31       129824     0.
        Linear Space     3              0            0     0.
          Dual Space    63             51        47736     0.
            FE Space     3              0            0     0.

--- Event Stage 2: libCEED Setup Stage

              Vector     4              0            0     0.

--- Event Stage 3: SNES Setup Stage

              Matrix     8              0            0     0.
           Index Set     4              4         3592     0.
   IS L to G Mapping     1              0            0     0.
             Section     4              4         2944     0.
   Star Forest Graph     6              5         5680     0.
         Vec Scatter     1              0            0     0.
              Vector     7              1         1696     0.
                SNES     2              0            0     0.
              DMSNES     2              0            0     0.
      SNESLineSearch     1              0            0     0.
       Krylov Solver     8              1         1480     0.
      Preconditioner     5              0            0     0.
              Viewer     1              0            0     0.

--- Event Stage 4: SNES Solve Stage

           Container    16             10         5840     0.
    Distributed Mesh    60             47       241120     0.
            DM Label   112             90        57600     0.
    GraphPartitioner    17             13         8944     0.
          Quadrature    86             35        21000     0.
              Matrix   473            451      9528844     0.
  Matrix FD Coloring     1              0            0     0.
     Matrix Coloring     1              1          656     0.
      Matrix Coarsen    12             12         7728     0.
           Index Set   813            670       651708     0.
             Section   108             81        59616     0.
   Star Forest Graph   234            212       226144     0.
     Discrete System    76             65        60840     0.
         Vec Scatter   102             98        79968     0.
              Vector   955            869      1524952     0.
        Linear Space     1              0            0     0.
          Dual Space    17             13        12168     0.
            FE Space     1              0            0     0.
       Krylov Solver    18             12       365760     0.
     DMKSP interface     1              0            0     0.
      Preconditioner    16             12        10368     0.
              Viewer     3              3         2544     0.
       Field over DM     1              0            0     0.
         PetscRandom    24             24        15696     0.
========================================================================================================================
Average time to get PetscTime(): 3.06e-08
Average time for MPI_Barrier(): 1.56196e-05
Average time for zero size MPI_Send(): 1.14464e-06
#PETSc Option Table entries:
-bc_clamp 998,999
-bc_clamp_998_translate 0,-2,0
-degree 3
-E 1e6
-log_view
-mesh ./meshes/cyl-hole_1994e_4ss_us.exo
-nu 0.3
-num_steps 3
-problem hyperSS
-snes_monitor
-snes_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-exodusii --download-hdf5 --download-hypre --download-metis --download-ml --download-netcdf --download-parmetis --download-pnetcdf --download-zlib --with-blaslapack-dir=/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64 --with-debugging=0 --with-fortran-bindings=0 --with-mpi-dir=/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0 COPTFLAGS="-Ofast -march=haswell -g" PETSC_ARCH=ompi-gcc8 --with-mpi-dir=/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0 COPTFLAGS="-O3 -ffp-contract=fast -march=haswell" PETSC_ARCH=ompi-gcc8-O3
-----------------------------------------
Libraries compiled on 2020-05-10 21:57:18 on shas0137 
Machine characteristics: Linux-3.10.0-957.21.3.el7.x86_64-x86_64-with-redhat-7.4-Maipo
Using PETSc directory: /projects/jeka2967/petsc
Using PETSc arch: ompi-gcc8-O3
-----------------------------------------

Using C compiler: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -ffp-contract=fast -march=haswell   
Using Fortran compiler: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O     
-----------------------------------------

Using include paths: -I/projects/jeka2967/petsc/include -I/projects/jeka2967/petsc/ompi-gcc8-O3/include -I/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64/../../include -I/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/include
-----------------------------------------

Using C linker: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpicc
Using Fortran linker: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpif90
Using libraries: -Wl,-rpath,/projects/jeka2967/petsc/ompi-gcc8-O3/lib -L/projects/jeka2967/petsc/ompi-gcc8-O3/lib -lpetsc -Wl,-rpath,/projects/jeka2967/petsc/ompi-gcc8-O3/lib -L/projects/jeka2967/petsc/ompi-gcc8-O3/lib -Wl,-rpath,/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64 -L/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64 -Wl,-rpath,/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/lib -L/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/lib -Wl,-rpath,/curc/sw/gcc/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0 -L/curc/sw/gcc/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0 -Wl,-rpath,/curc/sw/gcc/8.2.0/lib64 -L/curc/sw/gcc/8.2.0/lib64 -Wl,-rpath,/curc/sw/gcc/8.2.0/lib -L/curc/sw/gcc/8.2.0/lib -lHYPRE -lml -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lX11 -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lquadmath -lstdc++ -ldl
-----------------------------------------

