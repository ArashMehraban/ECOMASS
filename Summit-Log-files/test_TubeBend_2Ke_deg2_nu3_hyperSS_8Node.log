
-- Elastisticy Example - libCEED + PETSc --
  libCEED:
    libCEED Backend                    : /cpu/self/xsmm/blocked
  Problem:
    Problem Name                       : Hyper elasticity small strain
    Forcing Function                   : None
  Mesh:
    File                               : ./meshes/cyl-hole_1994e_4ss_us.exo
    Number of 1D Basis Nodes (p)       : 3
    Number of 1D Quadrature Points (q) : 3
    Global nodes                       : 22250
    Owned nodes                        : 26
    DoF per node                       : 3
  Multigrid:
    Type                               : P-multigrid, logarithmic coarsening
    Number of Levels                   : 2
    Level 0 (coarse):
      Number of 1D Basis Nodes (p)     : 2
      Global Nodes                     : 3212
      Owned Nodes                      : 0
    Level 1 (fine):
      Number of 1D Basis Nodes (p)     : 3
      Global Nodes                     : 22250
      Owned Nodes                      : 26
0 Load Increment
  0 SNES Function norm 5.776619768815e+05 
  1 SNES Function norm 2.353231315950e+01 
  2 SNES Function norm 2.890067948459e-04 
SNES Object: 192 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=75
  total number of function evaluations=5
  norm schedule ALWAYS
  SNESLineSearch Object: 192 MPI processes
    type: cp
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object: (outer_) 192 MPI processes
    type: cg
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using NATURAL norm type for convergence test
  PC Object: (outer_) 192 MPI processes
    type: mg
      type is MULTIPLICATIVE, levels=2 cycles=v
        Cycles per PCApply=1
        Not using Galerkin computed coarse grid matrices
    Coarse grid solver -- level -------------------------------
      KSP Object: (coarse_) 192 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (coarse_) 192 MPI processes
        type: gamg
          type is MULTIPLICATIVE, levels=3 cycles=v
            Cycles per PCApply=1
            Using externally compute Galerkin coarse grid matrices
            GAMG specific options
              Threshold for dropping small values in graph on each level =         0.        
              Threshold scaling factor for each level not specified = 1.
              AGG specific options
                Symmetric graph false
                Number of levels to square graph 1
                Number smoothing steps 1
              Complexity:    grid = 1.02563
        Coarse grid solver -- level -------------------------------
          KSP Object: (coarse_mg_coarse_) 192 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_coarse_) 192 MPI processes
            type: bjacobi
              number of blocks = 192
              Local solver is the same for all blocks, as in the following KSP and PC objects on rank 0:
            KSP Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 5., needed 1.13208
                  Factored matrix follows:
                    Mat Object: 1 MPI processes
                      type: seqaij
                      rows=36, cols=36, bs=3
                      package used to perform factorization: petsc
                      total: nonzeros=1080, allocated nonzeros=1080
                      total number of mallocs used during MatSetValues calls=0
                        using I-node routines: found 9 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=3
                total: nonzeros=954, allocated nonzeros=954
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 12 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=36, cols=36, bs=3
              total: nonzeros=954, allocated nonzeros=954
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 12 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (coarse_mg_levels_1_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.160398, max = 1.76438
              eigenvalues estimate via gmres min 0.0380405, max 1.60398
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_1_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_1_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=291, cols=291, bs=3
              total: nonzeros=14967, allocated nonzeros=14967
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 6 nodes, limit used is 5
        Up solver (post-smoother) same as down solver (pre-smoother)
        Down solver (pre-smoother) on level 2 -------------------------------
          KSP Object: (coarse_mg_levels_2_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.21775, max = 2.39525
              eigenvalues estimate via gmres min 0.0272134, max 2.1775
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_2_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_2_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=9636, cols=9636, bs=3
              total: nonzeros=621270, allocated nonzeros=621270
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: mpiaij
          rows=9636, cols=9636, bs=3
          total: nonzeros=621270, allocated nonzeros=621270
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
    Down solver (pre-smoother) on level 1 -------------------------------
      KSP Object: (outer_mg_levels_1_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.289309, max = 3.1824
          eigenvalues estimate via cg min 0.0505433, max 2.89309
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_1_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_1_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=66750, cols=66750
    Up solver (post-smoother) same as down solver (pre-smoother)
    linear system matrix = precond matrix:
    Mat Object: 192 MPI processes
      type: shell
      rows=66750, cols=66750
1 Load Increment
  0 SNES Function norm 5.776619768836e+05 
  1 SNES Function norm 2.356854136311e+01 
  2 SNES Function norm 2.971430723035e-04 
SNES Object: 192 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=76
  total number of function evaluations=5
  norm schedule ALWAYS
  SNESLineSearch Object: 192 MPI processes
    type: cp
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object: (outer_) 192 MPI processes
    type: cg
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using NATURAL norm type for convergence test
  PC Object: (outer_) 192 MPI processes
    type: mg
      type is MULTIPLICATIVE, levels=2 cycles=v
        Cycles per PCApply=1
        Not using Galerkin computed coarse grid matrices
    Coarse grid solver -- level -------------------------------
      KSP Object: (coarse_) 192 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (coarse_) 192 MPI processes
        type: gamg
          type is MULTIPLICATIVE, levels=3 cycles=v
            Cycles per PCApply=1
            Using externally compute Galerkin coarse grid matrices
            GAMG specific options
              Threshold for dropping small values in graph on each level =         0.        
              Threshold scaling factor for each level not specified = 1.
              AGG specific options
                Symmetric graph false
                Number of levels to square graph 1
                Number smoothing steps 1
              Complexity:    grid = 1.02563
        Coarse grid solver -- level -------------------------------
          KSP Object: (coarse_mg_coarse_) 192 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_coarse_) 192 MPI processes
            type: bjacobi
              number of blocks = 192
              Local solver is the same for all blocks, as in the following KSP and PC objects on rank 0:
            KSP Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 5., needed 1.13208
                  Factored matrix follows:
                    Mat Object: 1 MPI processes
                      type: seqaij
                      rows=36, cols=36, bs=3
                      package used to perform factorization: petsc
                      total: nonzeros=1080, allocated nonzeros=1080
                      total number of mallocs used during MatSetValues calls=0
                        using I-node routines: found 9 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=3
                total: nonzeros=954, allocated nonzeros=954
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 12 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=36, cols=36, bs=3
              total: nonzeros=954, allocated nonzeros=954
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 12 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (coarse_mg_levels_1_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.160395, max = 1.76435
              eigenvalues estimate via gmres min 0.0380396, max 1.60395
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_1_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_1_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=291, cols=291, bs=3
              total: nonzeros=14967, allocated nonzeros=14967
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 6 nodes, limit used is 5
        Up solver (post-smoother) same as down solver (pre-smoother)
        Down solver (pre-smoother) on level 2 -------------------------------
          KSP Object: (coarse_mg_levels_2_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.21766, max = 2.39426
              eigenvalues estimate via gmres min 0.0272045, max 2.1766
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_2_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_2_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=9636, cols=9636, bs=3
              total: nonzeros=621270, allocated nonzeros=621270
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: mpiaij
          rows=9636, cols=9636, bs=3
          total: nonzeros=621270, allocated nonzeros=621270
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
    Down solver (pre-smoother) on level 1 -------------------------------
      KSP Object: (outer_mg_levels_1_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.289329, max = 3.18262
          eigenvalues estimate via cg min 0.0505464, max 2.89329
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_1_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_1_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=66750, cols=66750
    Up solver (post-smoother) same as down solver (pre-smoother)
    linear system matrix = precond matrix:
    Mat Object: 192 MPI processes
      type: shell
      rows=66750, cols=66750
2 Load Increment
  0 SNES Function norm 5.776619768864e+05 
  1 SNES Function norm 2.363796781394e+01 
  2 SNES Function norm 3.119630871563e-04 
SNES Object: 192 MPI processes
  type: newtonls
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=77
  total number of function evaluations=5
  norm schedule ALWAYS
  SNESLineSearch Object: 192 MPI processes
    type: cp
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object: (outer_) 192 MPI processes
    type: cg
    maximum iterations=10000, initial guess is zero
    tolerances:  relative=1e-10, absolute=1e-50, divergence=10000.
    left preconditioning
    using NATURAL norm type for convergence test
  PC Object: (outer_) 192 MPI processes
    type: mg
      type is MULTIPLICATIVE, levels=2 cycles=v
        Cycles per PCApply=1
        Not using Galerkin computed coarse grid matrices
    Coarse grid solver -- level -------------------------------
      KSP Object: (coarse_) 192 MPI processes
        type: preonly
        maximum iterations=10000, initial guess is zero
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (coarse_) 192 MPI processes
        type: gamg
          type is MULTIPLICATIVE, levels=3 cycles=v
            Cycles per PCApply=1
            Using externally compute Galerkin coarse grid matrices
            GAMG specific options
              Threshold for dropping small values in graph on each level =         0.        
              Threshold scaling factor for each level not specified = 1.
              AGG specific options
                Symmetric graph false
                Number of levels to square graph 1
                Number smoothing steps 1
              Complexity:    grid = 1.02563
        Coarse grid solver -- level -------------------------------
          KSP Object: (coarse_mg_coarse_) 192 MPI processes
            type: preonly
            maximum iterations=10000, initial guess is zero
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_coarse_) 192 MPI processes
            type: bjacobi
              number of blocks = 192
              Local solver is the same for all blocks, as in the following KSP and PC objects on rank 0:
            KSP Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: preonly
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
              left preconditioning
              using NONE norm type for convergence test
            PC Object: (coarse_mg_coarse_sub_) 1 MPI processes
              type: lu
                out-of-place factorization
                tolerance for zero pivot 2.22045e-14
                using diagonal shift on blocks to prevent zero pivot [INBLOCKS]
                matrix ordering: nd
                factor fill ratio given 5., needed 1.13208
                  Factored matrix follows:
                    Mat Object: 1 MPI processes
                      type: seqaij
                      rows=36, cols=36, bs=3
                      package used to perform factorization: petsc
                      total: nonzeros=1080, allocated nonzeros=1080
                      total number of mallocs used during MatSetValues calls=0
                        using I-node routines: found 9 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object: 1 MPI processes
                type: seqaij
                rows=36, cols=36, bs=3
                total: nonzeros=954, allocated nonzeros=954
                total number of mallocs used during MatSetValues calls=0
                  using I-node routines: found 12 nodes, limit used is 5
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=36, cols=36, bs=3
              total: nonzeros=954, allocated nonzeros=954
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 12 nodes, limit used is 5
        Down solver (pre-smoother) on level 1 -------------------------------
          KSP Object: (coarse_mg_levels_1_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.160393, max = 1.76432
              eigenvalues estimate via gmres min 0.0380388, max 1.60393
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_1_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_1_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=291, cols=291, bs=3
              total: nonzeros=14967, allocated nonzeros=14967
              total number of mallocs used during MatSetValues calls=0
                using I-node (on process 0) routines: found 6 nodes, limit used is 5
        Up solver (post-smoother) same as down solver (pre-smoother)
        Down solver (pre-smoother) on level 2 -------------------------------
          KSP Object: (coarse_mg_levels_2_) 192 MPI processes
            type: chebyshev
              eigenvalue estimates used:  min = 0.217572, max = 2.39329
              eigenvalues estimate via gmres min 0.027196, max 2.17572
              eigenvalues estimated using gmres with translations  [0. 0.1; 0. 1.1]
              KSP Object: (coarse_mg_levels_2_esteig_) 192 MPI processes
                type: gmres
                  restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                  happy breakdown tolerance 1e-30
                maximum iterations=10, initial guess is zero
                tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
                left preconditioning
                using PRECONDITIONED norm type for convergence test
              estimating eigenvalues using noisy right hand side
            maximum iterations=2, nonzero initial guess
            tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
            left preconditioning
            using NONE norm type for convergence test
          PC Object: (coarse_mg_levels_2_) 192 MPI processes
            type: sor
              type = local_symmetric, iterations = 1, local iterations = 1, omega = 1.
            linear system matrix = precond matrix:
            Mat Object: 192 MPI processes
              type: mpiaij
              rows=9636, cols=9636, bs=3
              total: nonzeros=621270, allocated nonzeros=621270
              total number of mallocs used during MatSetValues calls=0
                not using I-node (on process 0) routines
        Up solver (post-smoother) same as down solver (pre-smoother)
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: mpiaij
          rows=9636, cols=9636, bs=3
          total: nonzeros=621270, allocated nonzeros=621270
          total number of mallocs used during MatSetValues calls=0
            not using I-node (on process 0) routines
    Down solver (pre-smoother) on level 1 -------------------------------
      KSP Object: (outer_mg_levels_1_) 192 MPI processes
        type: chebyshev
          eigenvalue estimates used:  min = 0.289365, max = 3.18302
          eigenvalues estimate via cg min 0.0505536, max 2.89365
          eigenvalues estimated using cg with translations  [0. 0.1; 0. 1.1]
          KSP Object: (outer_mg_levels_1_esteig_) 192 MPI processes
            type: cg
            maximum iterations=10, initial guess is zero
            tolerances:  relative=1e-12, absolute=1e-50, divergence=10000.
            left preconditioning
            using PRECONDITIONED norm type for convergence test
          estimating eigenvalues using noisy right hand side
        maximum iterations=3, nonzero initial guess
        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000.
        left preconditioning
        using NONE norm type for convergence test
      PC Object: (outer_mg_levels_1_) 192 MPI processes
        type: jacobi
        linear system matrix = precond matrix:
        Mat Object: 192 MPI processes
          type: shell
          rows=66750, cols=66750
    Up solver (post-smoother) same as down solver (pre-smoother)
    linear system matrix = precond matrix:
    Mat Object: 192 MPI processes
      type: shell
      rows=66750, cols=66750
  SNES:
    SNES Type                          : newtonls
    SNES Convergence                   : CONVERGED_FNORM_RELATIVE
    Number of Load Increments          : 3
    Completed Load Increments          : 3
    Total SNES Iterations              : 6
    Final rnorm                        : 3.119631e-04
  Linear Solver:
    KSP Type                           : cg
    PC Type                            : mg
  P-Multigrid:
    PCMG Type                          : MULTIPLICATIVE
    PCMG Cycle Type                    : v
    Coarse Solve:
      KSP Type                         : preonly
      PC Type                          : gamg
  Performance:
    SNES Solve Time                    : 1.07992 (1.07899) sec
    Strain Energy                      : -1.358800e+07
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

./elasticity on a ompi-gcc8-O3 named shas0102.rc.int.colorado.edu with 192 processors, by arme5062 Thu Nov 19 16:46:33 2020
Using Petsc Development GIT revision: v3.13.1-148-g80ec162  GIT Date: 2020-05-08 19:48:40 +0000

                         Max       Max/Min     Avg       Total 
Time (sec):           1.605e+00     1.008   1.601e+00
Objects:              6.021e+03     1.117   5.479e+03
Flop:                 7.283e+07     4.886   3.088e+07  5.928e+09
Flop/sec:             4.540e+07     4.884   1.928e+07  3.703e+09
MPI Messages:         9.860e+04     7.357   5.413e+04  1.039e+07
MPI Message Lengths:  2.917e+07     6.206   2.662e+02  2.767e+09
MPI Reductions:       3.909e+03     1.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total 
 0:      Main Stage: 8.7856e-03   0.5%  9.8505e+06   0.2%  2.031e+03   0.0%  2.530e+02        0.0%  5.000e+00   0.1% 
 1: DM and Vector Setup Stage: 4.8529e-01  30.3%  2.5343e+09  42.7%  5.765e+04   0.6%  1.441e+02        0.3%  1.550e+02   4.0% 
 2: libCEED Setup Stage: 3.1302e-03   0.2%  0.0000e+00   0.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0% 
 3: SNES Setup Stage: 2.2924e-02   1.4%  6.3855e+04   0.0%  3.164e+04   0.3%  8.346e+02        1.0%  3.000e+01   0.8% 
 4: SNES Solve Stage: 1.0807e+00  67.5%  3.3841e+09  57.1%  1.030e+07  99.1%  2.651e+02       98.7%  3.712e+03  95.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

MatMult               10 0.0 3.8160e-06 0.0 8.10e+02 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  2208
SFBcastOpBegin         1 1.0 2.1580e-0510.9 0.00e+00 0.0 1.3e+03 4.0e+02 0.0e+00  0  0  0  0  0   0  0 62 99  0     0
SFBcastOpEnd           1 1.0 3.2754e-04189.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   1  0  0  0  0     0
SFPack                 1 1.0 2.9670e-0626.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack               1 1.0 6.1180e-0651.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                 3 1.0 1.0952e-05 8.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 1: DM and Vector Setup Stage

BuildTwoSided         17 1.0 3.8991e-02 8.5 0.00e+00 0.0 1.2e+04 4.0e+00 1.7e+01  1  0  0  0  0   5  0 21  1 11     0
MatAssemblyBegin      58 1.0 3.5143e-0465.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd        58 1.0 1.6667e-04 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexCrFromFile       1 1.0 7.5013e-02 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 5.0e+00  4  0  0  0  0  15  0  0  0  3     0
Mesh Partition         1 1.0 3.0051e-01 1.0 0.00e+00 0.0 9.6e+02 9.2e+02 1.1e+01 19  0  0  0  0  62  0  2 11  7     0
Mesh Migration         1 1.0 1.5661e-02 1.0 0.00e+00 0.0 6.5e+03 6.3e+02 3.6e+01  1  0  0  0  1   3  0 11 49 23     0
DMPlexPartSelf         1 1.0 6.9659e-031138.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexPartLblInv       1 1.0 1.8378e-01 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 3.0e+00 11  0  0  0  0  37  0  0  2  2     0
DMPlexPartLblSF        1 1.0 1.6596e-0247.0 0.00e+00 0.0 3.8e+02 4.6e+02 1.0e+00  0  0  0  0  0   1  0  1  2  1     0
DMPlexPartStrtSF       1 1.0 1.2923e-03 1.1 0.00e+00 0.0 1.9e+02 1.8e+03 0.0e+00  0  0  0  0  0   0  0  0  4  0     0
DMPlexPointSF          1 1.0 2.9135e-03 1.1 0.00e+00 0.0 3.8e+02 2.3e+03 0.0e+00  0  0  0  0  0   1  0  1 11  0     0
DMPlexInterp          27 1.0 3.6622e-02 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.6e+01  2  0  0  0  0   7  0  0  0 10     0
DMPlexDistribute       1 1.0 3.1922e-01 1.0 0.00e+00 0.0 7.8e+03 7.5e+02 4.7e+01 20  0  0  0  1  66  0 14 70 30     0
DMPlexDistCones        1 1.0 2.9756e-03 1.4 0.00e+00 0.0 1.1e+03 1.4e+03 2.0e+00  0  0  0  0  0   1  0  2 19  1     0
DMPlexDistLabels       1 1.0 9.6998e-03 1.0 0.00e+00 0.0 3.8e+03 4.7e+02 3.1e+01  1  0  0  0  1   2  0  7 22 20     0
DMPlexDistField        1 1.0 2.9810e-03 1.2 0.00e+00 0.0 1.3e+03 4.0e+02 2.0e+00  0  0  0  0  0   1  0  2  6  1     0
DMPlexStratify        38 1.0 3.3072e-02 3.5 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+01  2  0  0  0  1   7  0  0  0 25     0
DMPlexSymmetrize      38 1.0 1.8692e-0345.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph            18 1.0 1.5576e-05 4.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp               17 1.0 4.0233e-02 7.5 0.00e+00 0.0 2.4e+04 6.8e+01 1.7e+01  2  0  0  0  0   5  0 42 20 11     0
SFBcastOpBegin        41 1.0 3.4792e-0326.1 0.00e+00 0.0 2.5e+04 2.1e+02 0.0e+00  0  0  0  0  0   0  0 44 63  0     0
SFBcastOpEnd          41 1.0 1.3379e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  1  0  0  0  0   2  0  0  0  0     0
SFReduceBegin          5 1.0 1.9600e-04 5.6 0.00e+00 0.0 5.3e+03 2.0e+02 0.0e+00  0  0  0  0  0   0  0  9 13  0     0
SFReduceEnd            5 1.0 3.7677e-0338.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFFetchOpBegin         1 1.0 1.9413e-0515.3 0.00e+00 0.0 1.3e+03 6.8e+01 0.0e+00  0  0  0  0  0   0  0  2  1  0     0
SFFetchOpEnd           1 1.0 1.3297e-0328.6 0.00e+00 0.0 1.3e+03 6.8e+01 0.0e+00  0  0  0  0  0   0  0  2  1  0     0
SFCreateEmbed          1 1.0 5.6863e-0481.6 0.00e+00 0.0 1.9e+02 2.3e+02 0.0e+00  0  0  0  0  0   0  0  0  1  0     0
SFDistSection         10 1.0 1.1704e-02 1.6 0.00e+00 0.0 1.8e+04 1.6e+02 1.2e+01  1  0  0  0  0   2  0 31 34  8     0
SFSectionSF           10 1.0 4.3337e-03 2.2 0.00e+00 0.0 1.1e+04 7.7e+01 1.0e+01  0  0  0  0  0   1  0 19 10  6     0
SFPack                47 1.0 1.4004e-03223.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              48 1.0 1.7142e-03240.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                38 1.0 4.6975e-05 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 2: libCEED Setup Stage

VecSet                 3 1.0 3.1790e-06 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 3: SNES Setup Stage

BuildTwoSided          7 1.0 6.1735e-03 1.8 0.00e+00 0.0 8.0e+03 4.0e+00 7.0e+00  0  0  0  0  0  20  0 25  0 23     0
BuildTwoSidedF         1 1.0 1.2702e-0343.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   5  0  0  0  3     0
MatAssemblyBegin       1 1.0 1.2935e-0323.4 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   5  0  0  0  3     0
MatAssemblyEnd         1 1.0 3.1240e-03 1.2 0.00e+00 0.0 4.2e+03 3.6e+01 5.0e+00  0  0  0  0  0  13  0 13  1 17     0
DMCreateMat            1 1.0 1.6246e-02 1.0 0.00e+00 0.0 2.7e+04 9.5e+02 1.9e+01  1  0  0  1  0  70  0 84 95 63     0
DMPlexPrealloc         1 1.0 1.5919e-02 1.0 0.00e+00 0.0 2.7e+04 9.5e+02 1.7e+01  1  0  0  1  0  68  0 84 95 57     0
SFSetGraph             8 1.0 3.8490e-06 3.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp                6 1.0 5.5444e-03 2.0 0.00e+00 0.0 1.6e+04 5.4e+02 6.0e+00  0  0  0  0  0  19  0 51 33 20     0
SFBcastOpBegin         6 1.0 2.8285e-0412.8 0.00e+00 0.0 7.3e+03 4.7e+02 0.0e+00  0  0  0  0  0   0  0 23 13  0     0
SFBcastOpEnd           6 1.0 1.6349e-03157.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   3  0  0  0  0     0
SFReduceBegin          5 1.0 8.8190e-0428.9 0.00e+00 0.0 5.9e+03 1.5e+03 0.0e+00  0  0  0  0  0   0  0 19 33  0     0
SFReduceEnd            5 1.0 2.8212e-03250.9 1.21e+03 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   4100  0  0  0    23
SFFetchOpBegin         1 1.0 6.4239e-0528.3 0.00e+00 0.0 1.2e+03 2.3e+03 0.0e+00  0  0  0  0  0   0  0  4 10  0     0
SFFetchOpEnd           1 1.0 1.2561e-0329.1 0.00e+00 0.0 1.2e+03 2.3e+03 0.0e+00  0  0  0  0  0   2  0  4 10  0     0
SFCreateEmbed          2 1.0 1.3088e-0329.8 0.00e+00 0.0 2.4e+03 1.7e+01 0.0e+00  0  0  0  0  0   2  0  8  0  0     0
SFSectionSF            2 1.0 2.1319e-03 8.1 0.00e+00 0.0 4.7e+03 6.0e+02 2.0e+00  0  0  0  0  0   6  0 15 11  7     0
SFRemoteOff            2 1.0 1.5008e-0316.2 0.00e+00 0.0 4.9e+03 4.3e+01 0.0e+00  0  0  0  0  0   3  0 15  1  0     0
SFPack                12 1.0 1.4397e-0423.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack              13 1.0 2.0913e-0416.4 1.21e+03 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0100  0  0  0   305
VecSet                 8 1.0 1.7949e-05 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0

--- Event Stage 4: SNES Solve Stage

PetscBarrier           2 1.0 9.9756e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  1  0  0  0  0   1  0  0  0  0     0
BuildTwoSided        367 1.0 1.2753e-01 1.7 0.00e+00 0.0 1.1e+05 4.0e+00 3.7e+02  7  0  1  0  9  11  0  1  0 10     0
BuildTwoSidedF       252 1.0 1.1780e-01 1.9 0.00e+00 0.0 4.3e+04 1.2e+03 2.5e+02  6  0  0  2  6  10  0  0  2  7     0
MatMult             4188 1.0 2.8358e-01 1.5 2.89e+07173.9 7.1e+06 3.1e+02 0.0e+00 14 25 68 79  0  21 44 69 80  0  5245
MatMultAdd           702 1.0 1.3339e-01 4.2 1.43e+06 6.7 6.6e+05 2.5e+02 0.0e+00  4  2  6  6  0   7  4  6  6  0   892
MatMultTranspose     709 1.0 8.0384e-02 6.6 1.02e+06 7.4 6.7e+05 2.5e+02 0.0e+00  2  1  6  6  0   3  2  7  6  0   989
MatSolve             234 0.0 3.8279e-04 0.0 4.97e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1298
MatSOR              2004 1.0 7.3418e-0328.0 1.62e+07 0.0 0.0e+00 0.0e+00 0.0e+00  0 10  0  0  0   0 17  0  0  0 78310
MatLUFactorSym         6 1.0 2.2179e-04 6.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatLUFactorNum         6 1.0 1.0826e-0417.2 1.21e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1121
MatScale              36 1.0 4.4401e-0356.5 3.24e+04 0.0 1.3e+04 4.6e+01 0.0e+00  0  0  0  0  0   0  0  0  0  0   432
MatResidual          702 1.0 4.9926e-02 2.7 6.37e+06245.1 1.1e+06 2.8e+02 0.0e+00  2  6 10 11  0   3 10 11 11  0  6576
MatAssemblyBegin     325 1.0 9.8332e-02 1.8 0.00e+00 0.0 4.3e+04 1.2e+03 1.1e+02  5  0  0  2  3   8  0  0  2  3     0
MatAssemblyEnd       325 1.0 6.6507e-02 1.1 4.77e+04 0.0 1.3e+05 1.5e+01 4.5e+02  4  0  1  0 12   6  0  1  0 12    37
MatGetRowIJ            7 7.0 1.1153e-03 2.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCreateSubMat       24 1.0 3.7717e-02 1.0 0.00e+00 0.0 7.1e+03 1.8e+02 3.4e+02  2  0  0  0  9   3  0  0  0  9     0
MatGetOrdering         6 0.0 1.8080e-04 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatCoarsen            12 1.0 2.1433e-02 1.1 0.00e+00 0.0 2.8e+05 3.2e+01 8.4e+01  1  0  3  0  2   2  0  3  0  2     0
MatZeroEntries        18 1.0 5.5598e-05 6.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView               24 1.3 6.2344e-02 3.3 0.00e+00 0.0 0.0e+00 0.0e+00 1.2e+01  4  0  0  0  0   5  0  0  0  0     0
MatAXPY               12 1.0 2.8363e-03 2.4 3.46e+03 0.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0  0    63
MatFDColorCreate       1 1.0 1.7900e-04 2.9 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatFDColorSetUp        1 1.0 2.9364e-02 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 4.2e+02  2  0  0  0 11   3  0  0  0 11     0
MatFDColorApply        6 1.0 8.1553e-02 1.0 5.63e+05 0.0 1.5e+06 1.4e+02 9.0e+00  5  0 14  7  0   8  1 14  8  0   308
MatFDColorFunc       630 1.0 8.0338e-02 1.0 3.21e+05 0.0 1.5e+06 1.4e+02 3.0e+00  5  0 14  7  0   7  0 14  8  0   161
MatTranspose          24 1.0 4.5280e-04 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatMatMultSym         36 1.0 2.0334e-02 1.0 0.00e+00 0.0 5.0e+04 1.7e+02 1.2e+02  1  0  0  0  3   2  0  0  0  3     0
MatMatMultNum         12 1.0 2.5262e-03 1.3 4.54e+05 0.0 1.3e+04 4.1e+02 1.2e+01  0  0  0  0  0   0  1  0  0  0  9067
MatPtAPSymbolic       12 1.0 2.0584e-02 1.1 0.00e+00 0.0 5.7e+04 4.9e+02 8.4e+01  1  0  1  1  2   2  0  1  1  2     0
MatPtAPNumeric        12 1.0 1.1071e-02 1.1 1.66e+06 0.0 2.4e+04 3.6e+02 7.2e+01  1  2  0  0  2   1  3  0  0  2  8254
MatTrnMatMultSym       6 1.0 1.2600e-02 1.1 0.00e+00 0.0 3.8e+04 2.8e+02 4.2e+01  1  0  0  0  1   1  0  0  0  1     0
MatTrnMatMultNum       6 1.0 2.1123e-02 1.1 4.63e+05 0.0 6.8e+04 5.9e+02 3.6e+01  1  0  1  1  1   2  1  1  1  1  1135
MatGetLocalMat        42 1.0 6.9121e-04 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetBrAoCol         36 1.0 6.7971e-0314.1 0.00e+00 0.0 8.9e+04 4.4e+02 0.0e+00  0  0  1  1  0   0  0  1  1  0     0
MatColoringApply       1 1.0 3.0152e-02 1.0 0.00e+00 0.0 1.9e+02 4.0e+00 3.0e+00  2  0  0  0  0   3  0  0  0  0     0
DMPlexInterp          13 1.0 4.5705e-03 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 7.0e+00  0  0  0  0  0   0  0  0  0  0     0
DMPlexStratify        17 1.0 3.1780e-03 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 1.7e+01  0  0  0  0  0   0  0  0  0  0     0
DMPlexSymmetrize      17 1.0 2.8847e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetGraph           115 1.0 3.4002e-05 2.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFSetUp              115 1.0 1.8748e-02 1.8 0.00e+00 0.0 1.7e+05 1.6e+01 1.2e+02  1  0  2  0  3   1  0  2  0  3     0
SFBcastOpBegin      5662 1.0 4.9522e-02 9.0 0.00e+00 0.0 6.4e+06 2.3e+02 0.0e+00  1  0 61 54  0   2  0 62 54  0     0
SFBcastOpEnd        5662 1.0 2.7994e-01 2.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00 13  0  0  0  0  19  0  0  0  0     0
SFReduceBegin       3286 1.0 2.6280e-02 5.4 0.00e+00 0.0 3.6e+06 3.2e+02 0.0e+00  1  0 34 41  0   1  0 35 42  0     0
SFReduceEnd         3286 1.0 1.8146e-0116.2 2.80e+06 0.0 0.0e+00 0.0e+00 0.0e+00  7  2  0  0  0  10  4  0  0  0   789
SFPack              8948 1.0 4.6526e-03 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SFUnpack            8948 1.0 5.0635e-03 3.6 2.80e+06 0.0 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  4  0  0  0 28265
VecDot                12 1.0 2.8394e-03 3.8 1.86e+0410.0 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0  0   563
VecMDot              240 1.0 3.1007e-02 1.5 2.53e+05 0.0 0.0e+00 0.0e+00 2.4e+02  2  0  0  0  6   2  0  0  0  6   418
VecTDot              588 1.0 9.3509e-02 2.1 9.10e+0510.0 0.0e+00 0.0e+00 5.9e+02  4  1  0  0 15   5  2  0  0 16   838
VecNorm              333 1.0 3.5602e-02 1.5 1.58e+0510.3 0.0e+00 0.0e+00 3.3e+02  2  0  0  0  9   3  0  0  0  9   332
VecScale            2437 1.0 3.1657e-04 1.7 2.53e+04 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  4139
VecCopy             2340 1.0 8.7348e-04 3.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet             11455 1.0 4.1030e-03 1.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             1491 1.0 1.2102e-03 3.6 1.54e+0611.8 0.0e+00 0.0e+00 0.0e+00  0  2  0  0  0   0  4  0  0  0 102558
VecAYPX             4956 1.0 1.9227e-03 3.3 2.60e+0610.0 0.0e+00 0.0e+00 0.0e+00  0  4  0  0  0   0  6  0  0  0 110068
VecAXPBYCZ          1872 1.0 9.1987e-04 3.0 4.07e+0610.0 0.0e+00 0.0e+00 0.0e+00  0  6  0  0  0   0 10  0  0  0 364855
VecMAXPY             264 1.0 1.3706e-04 2.7 3.00e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 112985
VecAssemblyBegin     156 1.0 2.0355e-02 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 1.4e+02  1  0  0  0  4   2  0  0  0  4     0
VecAssemblyEnd       156 1.0 6.0421e-04 7.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult    2083 1.0 1.2294e-03 2.7 1.52e+06 3.1 0.0e+00 0.0e+00 0.0e+00  0  3  0  0  0   0  5  0  0  0 131641
VecScatterBegin     3222 1.0 2.7848e-0217.4 0.00e+00 0.0 2.7e+06 1.2e+02 0.0e+00  1  0 26 12  0   1  0 27 13  0     0
VecScatterEnd       3222 1.0 1.2231e-01104.4 2.11e+05 0.0 0.0e+00 0.0e+00 0.0e+00  7  0  0  0  0  10  0  0  0  0     7
VecSetRandom          12 1.0 4.9057e-0515.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecReduceArith        18 1.0 8.4030e-06 1.9 2.78e+0410.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 285558
VecReduceComm          6 1.0 2.3009e-0310.3 0.00e+00 0.0 0.0e+00 0.0e+00 6.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNormalize         264 1.0 3.1297e-02 1.6 7.60e+04 0.0 0.0e+00 0.0e+00 2.6e+02  1  0  0  0  7   2  0  0  0  7   126
SNESSolve              3 1.0 9.8221e-01 1.0 5.96e+0734.9 1.0e+07 2.7e+02 3.7e+03 61 57 99 99 94  91100100100 99  3445
SNESSetUp              1 1.0 2.2751e-05 3.7 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
SNESFunctionEval     645 1.0 1.0688e-01 1.1 5.91e+0612008.7 1.5e+06 1.5e+02 3.8e+01  6  3 14  8  1  10  5 15  8  1  1514
SNESJacobianEval       6 1.0 1.4113e-01 1.0 5.63e+05 0.0 1.5e+06 1.4e+02 4.3e+02  9  0 14  7 11  13  1 14  8 12   178
SNESLineSearch         6 1.0 7.3740e-03 1.0 4.67e+06555.8 3.0e+04 4.0e+02 1.8e+01  0  2  0  0  0   1  4  0  0  0 17111
KSPSetUp              67 1.0 1.6923e-03 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 2.6e+01  0  0  0  0  1   0  0  0  0  1     0
KSPSolve               6 1.0 8.0967e-01 1.0 5.90e+0734.8 8.8e+06 2.9e+02 3.2e+03 51 54 85 91 82  75 95 85 92 86  3955
KSPGMRESOrthog       240 1.0 3.1175e-02 1.5 5.06e+05 0.0 0.0e+00 0.0e+00 2.4e+02  2  0  0  0  6   2  1  0  0  6   837
PCGAMGGraph_AGG       12 1.0 3.8189e-02 1.0 8.41e+04 0.0 6.4e+04 2.0e+01 2.3e+02  2  0  1  0  6   4  0  1  0  6   111
PCGAMGCoarse_AGG      12 1.0 6.8736e-02 1.0 4.63e+05 0.0 4.8e+05 1.4e+02 2.1e+02  4  0  5  2  5   6  1  5  2  6   349
PCGAMGProl_AGG        12 1.0 1.2509e-01 1.0 0.00e+00 0.0 2.3e+05 7.3e+01 4.8e+02  8  0  2  1 12  11  0  2  1 13     0
PCGAMGPOpt_AGG        12 1.0 7.1722e-02 1.0 2.30e+06 0.0 1.9e+05 1.7e+02 4.9e+02  4  2  2  1 13   7  3  2  1 13  1630
GAMG: createProl      12 1.0 3.0211e-01 1.0 2.85e+06 0.0 9.6e+05 1.2e+02 1.4e+03 19  2  9  4 36  28  4  9  4 38   480
  Graph               24 1.0 3.7847e-02 1.0 8.41e+04 0.0 6.4e+04 2.0e+01 2.3e+02  2  0  1  0  6   3  0  1  0  6   112
  MIS/Agg             12 1.0 2.1506e-02 1.1 0.00e+00 0.0 2.8e+05 3.2e+01 8.4e+01  1  0  3  0  2   2  0  3  0  2     0
  SA: col data        12 1.0 3.9834e-02 1.0 0.00e+00 0.0 2.1e+05 6.9e+01 3.6e+02  2  0  2  1  9   4  0  2  1 10     0
  SA: frmProl0        12 1.0 8.1397e-02 1.0 0.00e+00 0.0 1.1e+04 1.6e+02 7.2e+01  5  0  0  0  2   7  0  0  0  2     0
  SA: smooth          12 1.0 2.8290e-02 1.0 4.73e+05 0.0 6.3e+04 2.2e+02 1.7e+02  2  0  1  1  4   3  1  1  1  5   854
GAMG: partLevel       12 1.0 9.9064e-02 1.0 1.66e+06 0.0 9.1e+04 4.2e+02 7.9e+02  6  2  1  1 20   9  3  1  1 21   922
  repartition         12 1.0 6.8397e-02 1.0 0.00e+00 0.0 9.8e+03 1.4e+02 6.4e+02  4  0  0  0 16   6  0  0  0 17     0
  Invert-Sort         12 1.0 9.0485e-03 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 7.2e+01  1  0  0  0  2   1  0  0  0  2     0
  Move A              12 1.0 2.1165e-02 1.0 0.00e+00 0.0 2.5e+03 4.7e+02 1.8e+02  1  0  0  0  5   2  0  0  0  5     0
  Move P              12 1.0 2.0353e-02 1.0 0.00e+00 0.0 4.6e+03 2.2e+01 1.9e+02  1  0  0  0  5   2  0  0  0  5     0
PCSetUp               12 1.0 4.1542e-01 1.0 4.25e+061043.7 1.1e+06 1.5e+02 2.3e+03 26  4 10  6 59  38  7 10  6 62   573
PCSetUpOnBlocks      234 1.0 7.3644e-04 2.9 1.21e+05 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0   165
PCApply              234 1.0 3.3722e-01 1.1 5.27e+0736.7 7.1e+06 3.0e+02 4.5e+02 20 47 69 76 11  30 83 69 77 12  8296
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container     0             19        11096     0.
    Distributed Mesh     0             45       231856     0.
            DM Label     0             96        61440     0.
    GraphPartitioner     0             13         8944     0.
          Quadrature     0            231       138600     0.
              Matrix     0             42       210056     0.
  Matrix FD Coloring     0              1         2036     0.
           Index Set     8            221       204740     0.
   IS L to G Mapping     0              1          940     0.
             Section     0             92        67712     0.
   Star Forest Graph     0             67        67760     0.
     Discrete System     0             37        34640     0.
         Vec Scatter     0              5         4080     0.
              Vector     0            104       234992     0.
        Linear Space     0              3         2064     0.
          Dual Space     0             12        11232     0.
            FE Space     0              3         2340     0.
                SNES     0              2         2820     0.
              DMSNES     0              2         1360     0.
      SNESLineSearch     0              1         1000     0.
       Krylov Solver     0             11       363456     0.
     DMKSP interface     0              1          664     0.
      Preconditioner     0              8         8780     0.
              Viewer     1              1          848     0.
       Field over DM     0              1          712     0.

--- Event Stage 1: DM and Vector Setup Stage

           Container    35             22        12848     0.
    Distributed Mesh   131             99       508904     0.
            DM Label   281            207       132480     0.
    GraphPartitioner    38             29        19952     0.
          Quadrature   297            117        70200     0.
              Matrix    93             79       246924     0.
           Index Set  1156           1086      1198384     0.
   IS L to G Mapping     1              1        76312     0.
             Section   266            201       147936     0.
   Star Forest Graph   279            235       235280     0.
     Discrete System   168            142       132912     0.
              Vector    43             21       113936     0.
        Linear Space     2              0            0     0.
          Dual Space    40             32        29952     0.
            FE Space     2              0            0     0.

--- Event Stage 2: libCEED Setup Stage

              Vector     3              0            0     0.

--- Event Stage 3: SNES Setup Stage

              Matrix     6              0            0     0.
           Index Set     4              4         3592     0.
   IS L to G Mapping     1              0            0     0.
             Section     4              4         2944     0.
   Star Forest Graph     6              5         5680     0.
         Vec Scatter     1              0            0     0.
              Vector     5              1         1696     0.
                SNES     2              0            0     0.
              DMSNES     2              0            0     0.
      SNESLineSearch     1              0            0     0.
       Krylov Solver     6              1         1480     0.
      Preconditioner     4              0            0     0.
              Viewer     1              0            0     0.

--- Event Stage 4: SNES Solve Stage

           Container    16             10         5840     0.
    Distributed Mesh    60             47       241120     0.
            DM Label   112             90        57600     0.
    GraphPartitioner    17             13         8944     0.
          Quadrature    86             35        21000     0.
              Matrix   473            451      9528844     0.
  Matrix FD Coloring     1              0            0     0.
     Matrix Coloring     1              1          656     0.
      Matrix Coarsen    12             12         7728     0.
           Index Set   813            670       651708     0.
             Section   108             81        59616     0.
   Star Forest Graph   234            212       226144     0.
     Discrete System    76             65        60840     0.
         Vec Scatter   102             98        79968     0.
              Vector   941            866      1509352     0.
        Linear Space     1              0            0     0.
          Dual Space    17             13        12168     0.
            FE Space     1              0            0     0.
       Krylov Solver    18             12       365760     0.
     DMKSP interface     1              0            0     0.
      Preconditioner    16             12        10368     0.
              Viewer     3              3         2544     0.
       Field over DM     1              0            0     0.
         PetscRandom    24             24        15696     0.
========================================================================================================================
Average time to get PetscTime(): 3.03e-08
Average time for MPI_Barrier(): 1.76274e-05
Average time for zero size MPI_Send(): 1.06083e-06
#PETSc Option Table entries:
-bc_clamp 998,999
-bc_clamp_998_translate 0,-2,0
-degree 2
-E 1e6
-log_view
-mesh ./meshes/cyl-hole_1994e_4ss_us.exo
-nu 0.3
-num_steps 3
-problem hyperSS
-snes_monitor
-snes_view
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --download-exodusii --download-hdf5 --download-hypre --download-metis --download-ml --download-netcdf --download-parmetis --download-pnetcdf --download-zlib --with-blaslapack-dir=/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64 --with-debugging=0 --with-fortran-bindings=0 --with-mpi-dir=/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0 COPTFLAGS="-Ofast -march=haswell -g" PETSC_ARCH=ompi-gcc8 --with-mpi-dir=/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0 COPTFLAGS="-O3 -ffp-contract=fast -march=haswell" PETSC_ARCH=ompi-gcc8-O3
-----------------------------------------
Libraries compiled on 2020-05-10 21:57:18 on shas0137 
Machine characteristics: Linux-3.10.0-957.21.3.el7.x86_64-x86_64-with-redhat-7.4-Maipo
Using PETSc directory: /projects/jeka2967/petsc
Using PETSc arch: ompi-gcc8-O3
-----------------------------------------

Using C compiler: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpicc  -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -O3 -ffp-contract=fast -march=haswell   
Using Fortran compiler: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpif90  -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O     
-----------------------------------------

Using include paths: -I/projects/jeka2967/petsc/include -I/projects/jeka2967/petsc/ompi-gcc8-O3/include -I/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64/../../include -I/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/include
-----------------------------------------

Using C linker: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpicc
Using Fortran linker: /curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/bin/mpif90
Using libraries: -Wl,-rpath,/projects/jeka2967/petsc/ompi-gcc8-O3/lib -L/projects/jeka2967/petsc/ompi-gcc8-O3/lib -lpetsc -Wl,-rpath,/projects/jeka2967/petsc/ompi-gcc8-O3/lib -L/projects/jeka2967/petsc/ompi-gcc8-O3/lib -Wl,-rpath,/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64 -L/curc/sw/intel/parallel_studio_xe_2018_update3_cluster_edition/mkl/lib/intel64 -Wl,-rpath,/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/lib -L/curc/sw/openmpi_ucx/4.0.2/gcc/8.2.0/lib -Wl,-rpath,/curc/sw/gcc/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0 -L/curc/sw/gcc/8.2.0/lib/gcc/x86_64-pc-linux-gnu/8.2.0 -Wl,-rpath,/curc/sw/gcc/8.2.0/lib64 -L/curc/sw/gcc/8.2.0/lib64 -Wl,-rpath,/curc/sw/gcc/8.2.0/lib -L/curc/sw/gcc/8.2.0/lib -lHYPRE -lml -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lX11 -lexodus -lnetcdf -lpnetcdf -lhdf5hl_fortran -lhdf5_fortran -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lz -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lquadmath -lstdc++ -ldl
-----------------------------------------

